{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Start SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "print(os.environ.get(\"HADOOP_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"ivs\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark session:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/customs_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(PATH).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(PATH, sep=';', header=True)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.show(2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### PrintSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df\\\n",
    "    .withColumnRenamed(\"direction_eng\", \"direction\")\\\n",
    "    .withColumnRenamed(\"measure_eng\", \"measure\")\n",
    "\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "result\\\n",
    "    .select('country')\\\n",
    "    .distinct()\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    result\n",
    "    .groupBy('country').agg(F.count('*').alias('total_rows'))\n",
    "    .orderBy(F.col('total_rows').desc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de = (\n",
    "    result\n",
    "    .where(F.col('country') == 'DE')\n",
    "    .where(F.col('value').isNotNull())\n",
    ")\n",
    "\n",
    "print(df_de.count())\n",
    "\n",
    "# df_de2 = (\n",
    "#     result\n",
    "#     .where(''' country == \"DE\" ''')\n",
    "#     .where(''' value IS NOT NULL ''')\n",
    "# )\n",
    "\n",
    "# print(df_de.count() == df_de2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = (\n",
    "    df_de\n",
    "    .select(\n",
    "        'month',\n",
    "        'country',\n",
    "        'code',\n",
    "        'value',\n",
    "        'netto',\n",
    "        'quantity',\n",
    "        'region',\n",
    "        'district',\n",
    "        'direction',\n",
    "        'measure',\n",
    "        F.col('load_date').cast('date'),\n",
    "    )\n",
    ")\n",
    "\n",
    "final.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# This block of code demonstrates various methods for saving a DataFrame (final) \n",
    "# and controlling the number and structure of the output files.\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Uncontrolled saving by file count\n",
    "# (This will result in as many files as the current number of partitions in 'final' DataFrame)\n",
    "# (\n",
    "#     final\n",
    "#     .write\n",
    "#     .format('csv')\n",
    "#     .options(header='True', sep=';')\n",
    "#     .mode('overwrite')\n",
    "#     .csv('data/final_no_control')\n",
    "# )\n",
    "\n",
    "# partition_num = final.rdd.getNumPartitions()\n",
    "# print(f'Кол-во партиций {partition_num}') # Output: 'Кол-во партиций [number]'\n",
    "\n",
    "# # Controlled saving by file count - ONE FILE\n",
    "# # The coalesce(1) operation reduces the number of partitions to 1, forcing a single output file.\n",
    "# (\n",
    "#     final\n",
    "#     .coalesce(1)\n",
    "#     .write\n",
    "#     .format('csv')\n",
    "#     .options(header='True', sep=';')\n",
    "#     .csv('data/final_one_file')\n",
    "# )\n",
    "\n",
    "# partition_num = final.coalesce(1).rdd.getNumPartitions()\n",
    "# print(f'Кол-во партиций {partition_num}') # Output: 'Кол-во партиций 1'\n",
    "\n",
    "\n",
    "# # Saving with Partitioning\n",
    "# # Creates separate directories based on the distinct values in the 'load_date' column.\n",
    "# (\n",
    "#     final\n",
    "#     .write\n",
    "#     .partitionBy('load_date')\n",
    "#     .format('csv')\n",
    "#     .options(header='True', sep=';')\n",
    "#     .csv('data/final_partitioned')\n",
    "# )\n",
    "\n",
    "# print_df = final.select('load_date').distinct()\n",
    "# print(f'Load_date distinct: {print_df.count()}') # Output: 'Load_date distinct: [number]'\n",
    "\n",
    "\n",
    "# Saving with Partitioning and repartition within each partition (Controlled Partitioning)\n",
    "# The repartition(1, 'load_date') ensures that for each distinct 'load_date', \n",
    "# only ONE file is written within its corresponding directory, reducing file sprawl.\n",
    "(\n",
    "    final\n",
    "    .repartition(1, 'load_date')\n",
    "    .write\n",
    "    .partitionBy('load_date')\n",
    "    .format('csv')\n",
    "    .options(header='True', sep=';')\n",
    "    .csv('data/final_partitioned_repart')\n",
    ")\n",
    "\n",
    "# partition_num = final.repartition(1, 'load_date').rdd.getNumPartitions()\n",
    "# print(f'Кол-во партиций {partition_num}') # Output: 'Кол-во партиций [number of distinct load_dates]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Read Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_no_control = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_no_control/', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_final_one_file = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_one_file/', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_partitioned = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_partitioned', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_partitioned_repart = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_partitioned_repart', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "\n",
    "#reader_no_control.count() # number of files read: 16 | size of files read: 88.4 MiB | 2.5 s (90 ms, 301 ms, 384 ms)\n",
    "\n",
    "#reader_final_one_file.count() # number of files read: 1 | size of files read: 88.4 MiB | 3.2 s (306 ms, 407 ms, 420 ms )\n",
    "\n",
    "#reader_partitioned.count() # number of files read: 16 | size of files read: 16.4 MiB | 305 ms (32 ms, 39 ms, 54 ms )\n",
    "\n",
    "reader_partitioned_repart.count() # number of files read: 1 | size of files read: 16.4 MiB | 179 ms (9 ms, 43 ms, 44 ms )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (14000, \"Northern\"),\n",
    "    (11000, \"Southern\"),\n",
    "    (10000, \"Eastern\"),\n",
    "    (26000, \"Western\"),\n",
    "    (56000, \"Central\"),\n",
    "]\n",
    "region_df = spark.createDataFrame(data, schema='region_id long, name string')\n",
    "\n",
    "region_df.show()\n",
    "\n",
    "\n",
    "customs_data = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/customs_data.csv', header=True, sep=';')\n",
    ")\n",
    "\n",
    "customs_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off Broadcast JOIN\n",
    "import time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mo broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(region_df, customs_data.region==region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(F.broadcast(region_df), customs_data.region == region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for broadcast join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Cache | Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "customs_data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "customs_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "customs_data.persist(StorageLevel.DISK_ONLY).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Repartition & Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext.getConf().get(\"spark.driver.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1,'one'),\n",
    "    (2,'two'),\n",
    "    (3,'three'),\n",
    "    (4,'four'),\n",
    "    (5,'five'),\n",
    "    (6,'six'),\n",
    "    (7, 'seven'),\n",
    "    (8, 'eight'),\n",
    "    (9, 'nine'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'number'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.repartition(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.coalesce(2).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT OF MEMORY\n",
    "\n",
    "d = spark.read.csv('data/customs_data.csv', header=True, sep='\\t')\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
