{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dbd35c-c05b-4e74-9cb5-a3fc2a0e5351",
   "metadata": {},
   "source": [
    "### Start SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246fd58-0755-42e3-92d5-7767cbd48ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, sys\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "print(os.environ.get(\"HADOOP_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fcf1a3f-7641-4f37-9935-eb1cd80bf098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session: http://10090-LT-X0017.na.msds.rhi.com:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"halltape_pyspark_local\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark session:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81f1bb-00ec-4840-8998-0d2056e81e26",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2030acf5-9a3b-445b-87d3-9727ad4e3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/customs_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e72cd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|month;country;cod...|\n",
      "|01/2016;IT;620469...|\n",
      "|01/2016;CN;900190...|\n",
      "|01/2016;BY;841430...|\n",
      "|01/2016;US;901850...|\n",
      "|01/2016;EE;902110...|\n",
      "|01/2016;FR;381600...|\n",
      "|01/2016;MX;852351...|\n",
      "|01/2016;JP;620452...|\n",
      "|01/2016;KR;611020...|\n",
      "|01/2016;KG;852713...|\n",
      "|01/2016;ZA;842123...|\n",
      "|01/2016;CN;851810...|\n",
      "|01/2016;TR;841790...|\n",
      "|01/2016;IT;390610...|\n",
      "|01/2016;CZ;870840...|\n",
      "|01/2016;ES;640419...|\n",
      "|01/2016;IT;940490...|\n",
      "|01/2016;UA;820780...|\n",
      "|01/2016;CN;330410...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(PATH).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83700e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+------+--------+------+--------+-------------+-----------+-----------------------------+\n",
      "|month  |country|code      |value |netto |quantity|region|district|direction_eng|measure_eng|load_date                    |\n",
      "+-------+-------+----------+------+------+--------+------+--------+-------------+-----------+-----------------------------+\n",
      "|01/2016|IT     |6204695000|131   |1     |7       |46000 |01      |IM           |ShT        |2024-07-01T00:00:00.000+03:00|\n",
      "|01/2016|CN     |9001900009|112750|18    |0       |46000 |01      |IM           |1          |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|BY     |8414302004|392   |57    |8       |50000 |06      |IM           |ShT        |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|US     |9018509000|54349 |179   |0       |40000 |02      |IM           |1          |2024-04-01T00:00:00.000+03:00|\n",
      "|01/2016|EE     |9021101000|17304 |372   |0       |46000 |01      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|FR     |3816000000|323488|253600|0       |40000 |02      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|MX     |8523519300|1611  |0     |4       |40000 |02      |IM           |ShT        |2024-04-01T00:00:00.000+03:00|\n",
      "|01/2016|JP     |6204520000|29    |1     |2       |46000 |01      |IM           |ShT        |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|KR     |6110209100|815   |2     |5       |46000 |01      |IM           |ShT        |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|KG     |8527139900|11868 |2127  |2630    |46000 |01      |IM           |ShT        |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|ZA     |8421230000|12686 |1785  |3451    |45000 |01      |IM           |ShT        |2024-03-01T00:00:00.000+03:00|\n",
      "|01/2016|CN     |8518109500|12    |0     |10      |65000 |05      |IM           |ShT        |2024-03-01T00:00:00.000+03:00|\n",
      "|01/2016|TR     |8417900000|206453|17297 |1       |92000 |04      |IM           |ShT        |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|IT     |3906100000|4492  |1075  |0       |45000 |01      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|CZ     |8708409909|41    |2     |0       |46000 |01      |IM           |1          |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|ES     |6404191000|11822 |346   |760     |45000 |01      |IM           |PAR        |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|IT     |9404909000|6801  |485   |0       |46000 |01      |IM           |1          |2024-03-01T00:00:00.000+03:00|\n",
      "|01/2016|UA     |8207801900|35793 |1020  |0       |14000 |01      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|CN     |3304100000|59678 |10829 |0       |46000 |01      |IM           |1          |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|SI     |6104440000|1470  |13    |15      |45000 |01      |IM           |ShT        |2024-05-01T00:00:00.000+03:00|\n",
      "+-------+-------+----------+------+------+--------+------+--------+-------------+-----------+-----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-RECORD 0--------------------------------------\n",
      " month         | 01/2016                       \n",
      " country       | IT                            \n",
      " code          | 6204695000                    \n",
      " value         | 131                           \n",
      " netto         | 1                             \n",
      " quantity      | 7                             \n",
      " region        | 46000                         \n",
      " district      | 01                            \n",
      " direction_eng | IM                            \n",
      " measure_eng   | ShT                           \n",
      " load_date     | 2024-07-01T00:00:00.000+03:00 \n",
      "-RECORD 1--------------------------------------\n",
      " month         | 01/2016                       \n",
      " country       | CN                            \n",
      " code          | 9001900009                    \n",
      " value         | 112750                        \n",
      " netto         | 18                            \n",
      " quantity      | 0                             \n",
      " region        | 46000                         \n",
      " district      | 01                            \n",
      " direction_eng | IM                            \n",
      " measure_eng   | 1                             \n",
      " load_date     | 2024-01-01T00:00:00.000+03:00 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(PATH, sep=';', header=True)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.show(2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caf03e-fc30-4bc3-ac05-19fbc6336538",
   "metadata": {},
   "source": [
    "### PrintSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a90f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- netto: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- direction_eng: string (nullable = true)\n",
      " |-- measure_eng: string (nullable = true)\n",
      " |-- load_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf633ef-415f-417d-b3b8-fb56b5109029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'country',\n",
       " 'code',\n",
       " 'value',\n",
       " 'netto',\n",
       " 'quantity',\n",
       " 'region',\n",
       " 'district',\n",
       " 'direction',\n",
       " 'measure',\n",
       " 'load_date']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = df\\\n",
    "    .withColumnRenamed(\"direction_eng\", \"direction\")\\\n",
    "    .withColumnRenamed(\"measure_eng\", \"measure\")\n",
    "\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464d803",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afab5086-7d6b-460e-8954-b2b5da23cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|  month|country|      code| value|netto|quantity|region|district|direction|measure|           load_date|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|    1|       7| 46000|      01|       IM|    ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|   18|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "|01/2016|     BY|8414302004|   392|   57|       8| 50000|      06|       IM|    ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     US|9018509000| 54349|  179|       0| 40000|      02|       IM|      1|2024-04-01T00:00:...|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c67966ef-90cd-42d4-a260-236b60220312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|LT     |\n",
      "|MM     |\n",
      "|DZ     |\n",
      "|CI     |\n",
      "|TC     |\n",
      "|FI     |\n",
      "|SC     |\n",
      "|AZ     |\n",
      "|UA     |\n",
      "|RO     |\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result\\\n",
    "    .select('country')\\\n",
    "    .distinct()\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d080a-b5e8-45cb-aec0-8e03eb5e5bb7",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d94e8c7-41df-4a64-a0db-e302111f05d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|country|total_rows|\n",
      "+-------+----------+\n",
      "|     BY|   3509568|\n",
      "|     KZ|   2519896|\n",
      "|     CN|   2454792|\n",
      "|     DE|   1542311|\n",
      "|     UA|   1158498|\n",
      "|     IT|   1102837|\n",
      "|     US|    835936|\n",
      "|     PL|    666690|\n",
      "|     FR|    593040|\n",
      "|     JP|    571756|\n",
      "|     TR|    463432|\n",
      "|     KR|    446907|\n",
      "|     GB|    443091|\n",
      "|     AM|    438705|\n",
      "|     CZ|    407360|\n",
      "|     KG|    403565|\n",
      "|     ES|    401644|\n",
      "|     IN|    374151|\n",
      "|     NL|    365193|\n",
      "|     UZ|    329707|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    result\n",
    "    .groupBy('country').agg(F.count('*').alias('total_rows'))\n",
    "    .orderBy(F.col('total_rows').desc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f564628-8bf3-45cb-95f2-6f80ed4cec6a",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1cfb62c-0979-42fc-96ff-c5cd4eaba4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+------+--------+------+--------+---------+-------+--------------------+\n",
      "|  month|country|      code| value| netto|quantity|region|district|direction|measure|           load_date|\n",
      "+-------+-------+----------+------+------+--------+------+--------+---------+-------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|     1|       7| 46000|      01|       IM|    ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|    18|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "|01/2016|     BY|8414302004|   392|    57|       8| 50000|      06|       IM|    ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     US|9018509000| 54349|   179|       0| 40000|      02|       IM|      1|2024-04-01T00:00:...|\n",
      "|01/2016|     EE|9021101000| 17304|   372|       0| 46000|      01|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     FR|3816000000|323488|253600|       0| 40000|      02|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     MX|8523519300|  1611|     0|       4| 40000|      02|       IM|    ShT|2024-04-01T00:00:...|\n",
      "|01/2016|     JP|6204520000|    29|     1|       2| 46000|      01|       IM|    ShT|2024-02-01T00:00:...|\n",
      "|01/2016|     KR|6110209100|   815|     2|       5| 46000|      01|       IM|    ShT|2024-01-01T00:00:...|\n",
      "|01/2016|     KG|8527139900| 11868|  2127|    2630| 46000|      01|       IM|    ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     ZA|8421230000| 12686|  1785|    3451| 45000|      01|       IM|    ShT|2024-03-01T00:00:...|\n",
      "|01/2016|     CN|8518109500|    12|     0|      10| 65000|      05|       IM|    ShT|2024-03-01T00:00:...|\n",
      "|01/2016|     TR|8417900000|206453| 17297|       1| 92000|      04|       IM|    ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     IT|3906100000|  4492|  1075|       0| 45000|      01|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     CZ|8708409909|    41|     2|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "|01/2016|     ES|6404191000| 11822|   346|     760| 45000|      01|       IM|    PAR|2024-01-01T00:00:...|\n",
      "|01/2016|     IT|9404909000|  6801|   485|       0| 46000|      01|       IM|      1|2024-03-01T00:00:...|\n",
      "|01/2016|     UA|8207801900| 35793|  1020|       0| 14000|      01|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     CN|3304100000| 59678| 10829|       0| 46000|      01|       IM|      1|2024-06-01T00:00:...|\n",
      "|01/2016|     SI|6104440000|  1470|    13|      15| 45000|      01|       IM|    ShT|2024-05-01T00:00:...|\n",
      "+-------+-------+----------+------+------+--------+------+--------+---------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd2a20f-8af3-439a-a487-6c7d3fbcc64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1542311\n"
     ]
    }
   ],
   "source": [
    "df_de = (\n",
    "    result\n",
    "    .where(F.col('country') == 'DE')\n",
    "    .where(F.col('value').isNotNull())\n",
    ")\n",
    "\n",
    "print(df_de.count())\n",
    "\n",
    "# df_de2 = (\n",
    "#     result\n",
    "#     .where(''' country == \"DE\" ''')\n",
    "#     .where(''' value IS NOT NULL ''')\n",
    "# )\n",
    "\n",
    "# print(df_de.count() == df_de2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b564c-86f8-4633-a5ff-5cf8da7b6eea",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945d5e8c-c146-4221-9a29-9279d6911de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'country',\n",
       " 'code',\n",
       " 'value',\n",
       " 'netto',\n",
       " 'quantity',\n",
       " 'region',\n",
       " 'district',\n",
       " 'direction',\n",
       " 'measure',\n",
       " 'load_date']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_de.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62869a2-9781-428d-9292-6775b5bd5b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "|month  |country|code      |value|netto|quantity|region|district|direction|measure|load_date |\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "|01/2016|DE     |4016995709|5901 |172  |0       |46000 |01      |IM       |1      |2024-01-01|\n",
      "|01/2016|DE     |8708809109|1213 |94   |0       |45000 |01      |IM       |1      |2024-01-01|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = (\n",
    "    df_de\n",
    "    .select(\n",
    "        'month',\n",
    "        'country',\n",
    "        'code',\n",
    "        'value',\n",
    "        'netto',\n",
    "        'quantity',\n",
    "        'region',\n",
    "        'district',\n",
    "        'direction',\n",
    "        'measure',\n",
    "        F.col('load_date').cast('date'),\n",
    "    )\n",
    ")\n",
    "\n",
    "final.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b6c6d9-38bf-486d-9ecb-b1ffec0f5520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "|  month|country|      code|value|netto|quantity|region|district|direction|measure| load_date|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "|01/2016|     DE|4016995709| 5901|  172|       0| 46000|      01|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|8708809109| 1213|   94|       0| 45000|      01|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|7013419000| 7020| 1611|     492| 45000|      01|       IM|    ShT|2024-02-01|\n",
      "|01/2016|     DE|3923309090|46294| 8048|       0| 45000|      01|       IM|      1|2024-04-01|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff1058-85c4-4f3a-ad43-b3e69f41abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# This block of code demonstrates various methods for saving a DataFrame (final) \n",
    "# and controlling the number and structure of the output files.\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# Uncontrolled saving by file count\n",
    "# (This will result in as many files as the current number of partitions in 'final' DataFrame)\n",
    "# (\n",
    "#     final\n",
    "#     .write\n",
    "#     .format('csv')\n",
    "#     .options(header='True', sep=';')\n",
    "#     .mode('overwrite')\n",
    "#     .csv('data/final_no_control')\n",
    "# )\n",
    "\n",
    "# partition_num = final.rdd.getNumPartitions()\n",
    "# print(f'Кол-во партиций {partition_num}') # Output: 'Кол-во партиций [number]'\n",
    "\n",
    "# # Controlled saving by file count - ONE FILE\n",
    "# # The coalesce(1) operation reduces the number of partitions to 1, forcing a single output file.\n",
    "# (\n",
    "#     final\n",
    "#     .coalesce(1)\n",
    "#     .write\n",
    "#     .format('csv')\n",
    "#     .options(header='True', sep=';')\n",
    "#     .csv('data/final_one_file')\n",
    "# )\n",
    "\n",
    "# partition_num = final.coalesce(1).rdd.getNumPartitions()\n",
    "# print(f'Кол-во партиций {partition_num}') # Output: 'Кол-во партиций 1'\n",
    "\n",
    "\n",
    "# # Saving with Partitioning\n",
    "# # Creates separate directories based on the distinct values in the 'load_date' column.\n",
    "# (\n",
    "#     final\n",
    "#     .write\n",
    "#     .partitionBy('load_date')\n",
    "#     .format('csv')\n",
    "#     .options(header='True', sep=';')\n",
    "#     .csv('data/final_partitioned')\n",
    "# )\n",
    "\n",
    "# print_df = final.select('load_date').distinct()\n",
    "# print(f'Load_date distinct: {print_df.count()}') # Output: 'Load_date distinct: [number]'\n",
    "\n",
    "\n",
    "# Saving with Partitioning and repartition within each partition (Controlled Partitioning)\n",
    "# The repartition(1, 'load_date') ensures that for each distinct 'load_date', \n",
    "# only ONE file is written within its corresponding directory, reducing file sprawl.\n",
    "(\n",
    "    final\n",
    "    .repartition(1, 'load_date')\n",
    "    .write\n",
    "    .partitionBy('load_date')\n",
    "    .format('csv')\n",
    "    .options(header='True', sep=';')\n",
    "    .csv('data/final_partitioned_repart')\n",
    ")\n",
    "\n",
    "# partition_num = final.repartition(1, 'load_date').rdd.getNumPartitions()\n",
    "# print(f'Кол-во партиций {partition_num}') # Output: 'Кол-во партиций [number of distinct load_dates]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f71e8-ab17-4104-85d1-fc55f3f9a43f",
   "metadata": {},
   "source": [
    "### Read Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70e3d82e-b20d-4e3d-8ad9-e752662da308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_no_control = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_no_control/', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_final_one_file = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_one_file/', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_partitioned = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_partitioned', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_partitioned_repart = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_partitioned_repart', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "\n",
    "#reader_no_control.count() # number of files read: 16 | size of files read: 88.4 MiB | 2.5 s (90 ms, 301 ms, 384 ms)\n",
    "\n",
    "#reader_final_one_file.count() # number of files read: 1 | size of files read: 88.4 MiB | 3.2 s (306 ms, 407 ms, 420 ms )\n",
    "\n",
    "#reader_partitioned.count() # number of files read: 16 | size of files read: 16.4 MiB | 305 ms (32 ms, 39 ms, 54 ms )\n",
    "\n",
    "reader_partitioned_repart.count() # number of files read: 1 | size of files read: 16.4 MiB | 179 ms (9 ms, 43 ms, 44 ms )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f5468",
   "metadata": {},
   "source": [
    "### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (14000, \"Northern\"),\n",
    "    (11000, \"Southern\"),\n",
    "    (10000, \"Eastern\"),\n",
    "    (26000, \"Western\"),\n",
    "    (56000, \"Central\"),\n",
    "]\n",
    "region_df = spark.createDataFrame(data, schema='region_id long, name string')\n",
    "\n",
    "region_df.show()\n",
    "\n",
    "\n",
    "customs_data = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/customs_data.csv', header=True, sep=';')\n",
    ")\n",
    "\n",
    "customs_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5939bc3f-1bb0-4240-afff-da26e9818381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off Broadcast JOIN\n",
    "import time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42d681ee-cd9b-4caa-8dc3-6b1853f03a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for join operation: 27.30 seconds\n"
     ]
    }
   ],
   "source": [
    "# mo broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(region_df, customs_data.region==region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b661180-bcac-439b-a2bd-df3b1f8573c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for broadcast join operation: 19.21 seconds\n"
     ]
    }
   ],
   "source": [
    "# with broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(F.broadcast(region_df), customs_data.region == region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for broadcast join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9bc17",
   "metadata": {},
   "source": [
    "### Cache | Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c6344db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26392290"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customs_data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2be3547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, country: string, code: string, value: string, netto: string, quantity: string, region: string, district: string, direction_eng: string, measure_eng: string, load_date: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customs_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eedfda2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26392290"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "customs_data.persist(StorageLevel.DISK_ONLY).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9566a5",
   "metadata": {},
   "source": [
    "### Repartition & Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "172ef269-9d21-4b47-9818-33fa786187d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().get(\"spark.driver.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39c4a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|number|\n",
      "+---+------+\n",
      "|  1|   one|\n",
      "|  2|   two|\n",
      "|  3| three|\n",
      "|  4|  four|\n",
      "|  5|  five|\n",
      "|  6|   six|\n",
      "|  7| seven|\n",
      "|  8| eight|\n",
      "|  9|  nine|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,'one'),\n",
    "    (2,'two'),\n",
    "    (3,'three'),\n",
    "    (4,'four'),\n",
    "    (5,'five'),\n",
    "    (6,'six'),\n",
    "    (7, 'seven'),\n",
    "    (8, 'eight'),\n",
    "    (9, 'nine'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'number'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd3cf563-cf22-41ee-90f8-581e5705411b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "978ee041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=1, number='one'),\n",
       "  Row(id=4, number='four'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=9, number='nine')],\n",
       " [Row(id=2, number='two')],\n",
       " [Row(id=3, number='three'),\n",
       "  Row(id=5, number='five'),\n",
       "  Row(id=6, number='six'),\n",
       "  Row(id=8, number='eight')]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.repartition(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1eb8bba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=3, number='three'),\n",
       "  Row(id=6, number='six'),\n",
       "  Row(id=5, number='five'),\n",
       "  Row(id=9, number='nine')],\n",
       " [Row(id=7, number='seven'),\n",
       "  Row(id=1, number='one'),\n",
       "  Row(id=2, number='two'),\n",
       "  Row(id=8, number='eight'),\n",
       "  Row(id=4, number='four')]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.coalesce(2).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "255abe6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Pandas >= 1.0.5 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:27\u001b[39m, in \u001b[36mrequire_minimum_pandas_version\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\n\u001b[32m     29\u001b[39m     have_pandas = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:86\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     90\u001b[39m jconf = \u001b[38;5;28mself\u001b[39m.sparkSession._jconf\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\utils.py:34\u001b[39m, in \u001b[36mrequire_minimum_pandas_version\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m     raised_error = error\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_pandas:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m must be installed; however, \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mit was not found.\u001b[39m\u001b[33m\"\u001b[39m % minimum_pandas_version\n\u001b[32m     36\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mraised_error\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m must be installed; however, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % (minimum_pandas_version, pandas.__version__)\n\u001b[32m     41\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: Pandas >= 1.0.5 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "mix.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e010c0e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o393.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 97.0 failed 1 times, most recent failure: Lost task 6.0 in stage 97.0 (TID 536) (10090-LT-X0017.na.msds.rhi.com executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\r\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$2838/0x000002352ce0af28.apply(Unknown Source)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\r\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\r\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$$$Lambda$2841/0x000002352ce0c5b0.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\r\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$2840/0x000002352ce0c1d8.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\r\n\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572)\r\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1381)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\r\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\r\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$2838/0x000002352ce0af28.apply(Unknown Source)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\r\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\r\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$$$Lambda$2841/0x000002352ce0c5b0.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\r\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$2840/0x000002352ce0c1d8.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\r\n\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572)\r\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1381)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\r\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# OUT OF MEMORY\u001b[39;00m\n\u001b[32m      3\u001b[39m d = spark.read.csv(\u001b[33m'\u001b[39m\u001b[33mdata/customs_data.csv\u001b[39m\u001b[33m'\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, sep=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1261\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1241\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1242\u001b[39m \n\u001b[32m   1243\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1258\u001b[39m \u001b[33;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1259\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1260\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1261\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o393.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 97.0 failed 1 times, most recent failure: Lost task 6.0 in stage 97.0 (TID 536) (10090-LT-X0017.na.msds.rhi.com executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\r\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$2838/0x000002352ce0af28.apply(Unknown Source)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\r\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\r\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$$$Lambda$2841/0x000002352ce0c5b0.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\r\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$2840/0x000002352ce0c1d8.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\r\n\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572)\r\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1381)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\r\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\r\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$.$anonfun$serializeToChunkedBuffer$1$adapted(SerializerHelper.scala:40)\r\n\tat org.apache.spark.serializer.SerializerHelper$$$Lambda$2838/0x000002352ce0af28.apply(Unknown Source)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\r\n\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\r\n\tat java.base/java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1862)\r\n\tat java.base/java.io.ObjectOutputStream.write(ObjectOutputStream.java:714)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$.$anonfun$writeByteBuffer$1$adapted(Utils.scala:209)\r\n\tat org.apache.spark.util.Utils$$$Lambda$2841/0x000002352ce0c5b0.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.writeByteBufferImpl(Utils.scala:187)\r\n\tat org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:209)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$writeExternal$2$adapted(ChunkedByteBuffer.scala:103)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer$$Lambda$2840/0x000002352ce0c1d8.apply(Unknown Source)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat org.apache.spark.util.io.ChunkedByteBuffer.writeExternal(ChunkedByteBuffer.scala:103)\r\n\tat java.base/java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1465)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1436)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572)\r\n\tat java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529)\r\n\tat java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)\r\n\tat java.base/java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1381)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\r\n\tat java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350)\r\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59979)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ivazhu01\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# OUT OF MEMORY\n",
    "\n",
    "d = spark.read.csv('data/customs_data.csv', header=True, sep='\\t')\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bb1f01e-5698-432a-b725-78ef3c063e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:503\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:506\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    505\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mError while sending or receiving.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    507\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError while sending\u001b[39m\u001b[33m\"\u001b[39m, e, proto.ERROR_ON_SEND)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1796\u001b[39m, in \u001b[36mSparkSession.stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[33;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1792\u001b[39m \u001b[33;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1793\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[32m-> \u001b[39m\u001b[32m1796\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[32m   1798\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:654\u001b[39m, in \u001b[36mSparkContext.stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_jsc\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[32m    656\u001b[39m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[32m    657\u001b[39m         warnings.warn(\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m It is possible that the process has crashed,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    660\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m been killed or may also be in a zombie state.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    661\u001b[39m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[32m    662\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1053\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(retry, connection, pne):\n\u001b[32m   1052\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1055\u001b[39m     logging.exception(\n\u001b[32m   1056\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
